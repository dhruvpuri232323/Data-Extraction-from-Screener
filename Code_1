import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from urllib.parse import urljoin
import time

def extract_breadcrumb_hierarchy(soup):
    """
    Extracts the breadcrumb navigation hierarchy from the page
    """
    hierarchy = {
        'Industry': '',
        'Driving_Category': '',
        'Category': '',
        'Sector': '',
        'Sub_Category': ''
    }
    try:
        # Find the breadcrumb navigation bar
        breadcrumb_bar = soup.find('nav') or soup.find('div', attrs={'aria-label': 'breadcrumb'})
        categories = []
        if breadcrumb_bar:
            breadcrumb_links = breadcrumb_bar.find_all('a')
            categories = [item.get_text(strip=True) for item in breadcrumb_links]
        # Remove navigation links
        categories = [cat for cat in categories if cat.lower() not in ['industries', 'home', 'screens', 'login']]
        # Only keep the last 5 items (the hierarchy)
        if len(categories) > 5:
            categories = categories[-5:]
        # Map the categories to our hierarchy structure
        if len(categories) >= 1:
            hierarchy['Industry'] = categories[0]
        if len(categories) >= 2:
            hierarchy['Driving_Category'] = categories[1]
        if len(categories) >= 3:
            hierarchy['Category'] = categories[2]
        if len(categories) >= 4:
            hierarchy['Sector'] = categories[3]
        if len(categories) >= 5:
            hierarchy['Sub_Category'] = categories[4]
    except Exception as e:
        print(f"Error extracting hierarchy: {str(e)}")
    return hierarchy

def scrape_screener_industries():
    """
    Scrapes all industry links and their hierarchical categorization from screener.in/market/
    """
    
    # Base URL
    base_url = "https://www.screener.in"
    market_url = "https://www.screener.in/market/"
    
    # Headers to mimic a real browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        # Send GET request to the market page
        print("Fetching data from screener.in/market/...")
        response = requests.get(market_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parse the HTML content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find the table containing industry data
        industry_data = []
        industry_links = soup.find_all('a', href=re.compile(r'/market/'))
        
        print(f"Found {len(industry_links)} potential industry links...")
        
        # Filter out the column header links and get only industry links
        seen_industries = set()
        
        for link in industry_links:
            href = link.get('href')
            industry_name = link.get_text(strip=True)
            
            # Skip if it's a sorting/header link
            if '?' in href:
                continue
                
            # Skip if it's not an industry link
            path_segments = href.strip('/').split('/')
            if len(path_segments) < 4:
                continue
                
            # Skip empty or very short names
            if not industry_name or len(industry_name) < 2:
                continue
                
            # Create full URL
            full_url = urljoin(base_url, href)
            
            # Avoid duplicates
            if industry_name not in seen_industries:
                seen_industries.add(industry_name)
                
                # Visit the industry page to extract hierarchy
                try:
                    print(f"Fetching details for: {industry_name}")
                    industry_response = requests.get(full_url, headers=headers, timeout=30)
                    industry_soup = BeautifulSoup(industry_response.content, 'html.parser')
                    
                    # Extract hierarchy information
                    hierarchy = extract_breadcrumb_hierarchy(industry_soup)
                    
                    # Combine all information
                    industry_info = {
                        'Industry_Name': industry_name,
                        'Industry_Link': full_url,
                        **hierarchy  # Unpack hierarchy dictionary
                    }
                    
                    industry_data.append(industry_info)
                    print(f"Added: {industry_name}")
                    
                    # Be nice to the server
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"Error processing {industry_name}: {str(e)}")
                    # Add basic information even if detailed scraping fails
                    industry_data.append({
                        'Industry_Name': industry_name,
                        'Industry_Link': full_url,
                        'Industry': '',
                        'Driving_Category': '',
                        'Category': '',
                        'Sector': '',
                        'Sub_Category': ''
                    })
        
        # Sort by industry name
        industry_data.sort(key=lambda x: x['Industry_Name'])
        
        # Create DataFrame
        df = pd.DataFrame(industry_data)
        
        # Add serial number and reorder columns
        df['Serial_No'] = range(1, len(df) + 1)
        columns = ['Serial_No', 'Industry_Name', 'Industry', 'Driving_Category', 
                  'Category', 'Sector', 'Sub_Category', 'Industry_Link']
        df = df[columns]
        
        # Save to Excel file
        excel_filename = 'screener_industry_categorization.xlsx'
        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Industry_Categories', index=False)
            
            # Get the workbook and worksheet objects
            workbook = writer.book
            worksheet = writer.sheets['Industry_Categories']
            
            # Adjust column widths
            column_widths = {
                'A': 12,  # Serial_No
                'B': 40,  # Industry_Name
                'C': 30,  # Industry
                'D': 30,  # Driving_Category
                'E': 30,  # Category
                'F': 30,  # Sector
                'G': 30,  # Sub_Category
                'H': 80,  # Industry_Link
            }
            
            for col, width in column_widths.items():
                worksheet.column_dimensions[col].width = width
            
            # Add formatting
            from openpyxl.styles import Font, PatternFill
            
            header_font = Font(bold=True, color='FFFFFF')
            header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
            
            for cell in worksheet[1]:
                cell.font = header_font
                cell.fill = header_fill
        
        print(f"\n✅ Successfully scraped {len(industry_data)} industries with categorization!")
        print(f"📊 Data saved to: {excel_filename}")
        print("\n📋 Summary:")
        print(f"   • Total industries found: {len(industry_data)}")
        print(f"   • Excel file: {excel_filename}")
        print(f"   • Columns: {', '.join(columns)}")
        
        # Display preview
        print("\n🔍 Preview of first 3 industries with categories:")
        preview_df = df.head(3)
        for _, row in preview_df.iterrows():
            print(f"\n{int(row['Serial_No'])}. {row['Industry_Name']}")
            print(f"   Industry: {row['Industry']}")
            print(f"   Driving Category: {row['Driving_Category']}")
            print(f"   Category: {row['Category']}")
            print(f"   Sector: {row['Sector']}")
            print(f"   Sub-Category: {row['Sub_Category']}")
            print(f"   Link: {row['Industry_Link']}")
        
        return df
        
    except requests.exceptions.RequestException as e:
        print(f"❌ Error fetching data: {e}")
        return None
    except Exception as e:
        print(f"❌ Error processing data: {e}")
        return None

def verify_sample_links(df, sample_size=5):
    """
    Verify a sample of scraped links to ensure they work
    """
    import random
    
    if df is None or len(df) == 0:
        print("No data to verify")
        return
    
    print(f"\n🔍 Verifying {sample_size} random industry links...")
    
    # Select random sample
    sample_indices = random.sample(range(len(df)), min(sample_size, len(df)))
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for i, idx in enumerate(sample_indices):
        industry = df.iloc[idx]
        print(f"\n{i+1}. Testing: {industry['Industry_Name']}")
        print(f"   URL: {industry['Industry_Link']}")
        print(f"   Categories: {industry['Industry']} > {industry['Driving_Category']} > {industry['Category']} > {industry['Sector']} > {industry['Sub_Category']}")
        
        try:
            response = requests.get(industry['Industry_Link'], headers=headers, timeout=10)
            if response.status_code == 200:
                print("   ✅ Link works!")
            else:
                print(f"   ❌ Status code: {response.status_code}")
        except Exception as e:
            print(f"   ❌ Error: {str(e)[:50]}...")

if __name__ == "__main__":
    # Run the scraper
    print("🚀 Starting Screener.in Industry Categories Scraper...")
    print("=" * 60)
    
    # Scrape the data
    df = scrape_screener_industries()
    
    # Verify some links
    if df is not None:
        verify_sample_links(df, sample_size=3)
    
    print("\n" + "=" * 60)
    print("✨ Scraping completed!")
