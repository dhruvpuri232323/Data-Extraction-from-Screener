import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from urllib.parse import urljoin
import time

def extract_breadcrumb_hierarchy(soup, debug=False):
    """
    Extracts the 5-level breadcrumb navigation hierarchy from the page.
    Expected structure: Industries > Consumer Discretionary > Automobile and Auto Components > Automobiles > 2/3 Wheelers
    """
    hierarchy = {
        'Industry': 'Industries',  # Always "Industries"
        'Driving_Category': '',  # e.g., Consumer Discretionary
        'Category': '',  # e.g., Automobile and Auto Components
        'Sector': '',  # e.g., Automobiles
        'Sub_Category': ''   # e.g., 2/3 Wheelers
    }
    
    if debug:
        print("\nüîç DEBUG: Starting breadcrumb extraction...")
    
    try:
        # Method 1: Look for breadcrumb navigation containers
        breadcrumb_container = None
        
        # Try different possible breadcrumb selectors
        selectors = [
            'nav',
            'div[aria-label*="breadcrumb"]',
            '.breadcrumb',
            'ol.breadcrumb',
            'ul.breadcrumb',
            'div.breadcrumb',
            '.nav-breadcrumb',
            '.breadcrumbs'
        ]
        
        for selector in selectors:
            containers = soup.select(selector)
            for container in containers:
                if 'industries' in container.get_text().lower():
                    breadcrumb_container = container
                    if debug:
                        print(f"Found breadcrumb container using selector: {selector}")
                    break
            if breadcrumb_container:
                break
        
        # Method 2: Find "Industries" link and work from there
        if not breadcrumb_container:
            industries_links = soup.find_all('a', string=re.compile(r'Industries', re.IGNORECASE))
            for industries_link in industries_links:
                # Check if this link is part of a breadcrumb (has siblings with links)
                parent = industries_link.find_parent()
                if parent and len(parent.find_all('a')) > 1:
                    breadcrumb_container = parent
                    if debug:
                        print("Found breadcrumb container via Industries link")
                    break
                # Try going up one more level
                grandparent = parent.find_parent() if parent else None
                if grandparent and len(grandparent.find_all('a')) > 1:
                    breadcrumb_container = grandparent
                    if debug:
                        print("Found breadcrumb container via Industries link (grandparent)")
                    break
        
        if breadcrumb_container:
            # Extract all links from breadcrumb
            breadcrumb_links = breadcrumb_container.find_all('a')
            categories = []
            
            for link in breadcrumb_links:
                text = link.get_text(strip=True)
                if text and text.lower() not in ['home', 'login', 'screens', 'dashboard']:
                    categories.append(text)
            
            if debug:
                print(f"Found breadcrumb links: {categories}")
            
            # Find "Industries" and extract hierarchy after it
            industries_index = -1
            for i, cat in enumerate(categories):
                if cat.lower() == 'industries':
                    industries_index = i
                    break
            
            if industries_index >= 0:
                # Extract categories after "Industries"
                remaining_categories = categories[industries_index + 1:]
                if debug:
                    print(f"Categories after 'Industries': {remaining_categories}")
                
                # Map to hierarchy levels
                if len(remaining_categories) >= 1:
                    hierarchy['Driving_Category'] = remaining_categories[0]
                if len(remaining_categories) >= 2:
                    hierarchy['Category'] = remaining_categories[1]
                if len(remaining_categories) >= 3:
                    hierarchy['Sector'] = remaining_categories[2]
                if len(remaining_categories) >= 4:
                    hierarchy['Sub_Category'] = remaining_categories[3]
        
        # Method 3: Look for breadcrumb separators in page text
        if not any(hierarchy.values()[1:]):  # If we didn't extract anything
            if debug:
                print("Trying text pattern matching...")
            
            # Look for text patterns that might indicate breadcrumbs
            page_text = soup.get_text()
            
            # Try different separator patterns
            breadcrumb_patterns = [
                r'Industries\s*[>‚Ä∫/]\s*([^>‚Ä∫/\n]+)\s*[>‚Ä∫/]\s*([^>‚Ä∫/\n]+)\s*[>‚Ä∫/]\s*([^>‚Ä∫/\n]+)\s*[>‚Ä∫/]\s*([^>‚Ä∫/\n]+)',
                r'Industries\s*¬ª\s*([^¬ª\n]+)\s*¬ª\s*([^¬ª\n]+)\s*¬ª\s*([^¬ª\n]+)\s*¬ª\s*([^¬ª\n]+)',
                r'Industries\s*‚Üí\s*([^‚Üí\n]+)\s*‚Üí\s*([^‚Üí\n]+)\s*‚Üí\s*([^‚Üí\n]+)\s*‚Üí\s*([^‚Üí\n]+)',
            ]
            
            for pattern in breadcrumb_patterns:
                match = re.search(pattern, page_text)
                if match:
                    hierarchy['Driving_Category'] = match.group(1).strip()
                    hierarchy['Category'] = match.group(2).strip()
                    hierarchy['Sector'] = match.group(3).strip()
                    hierarchy['Sub_Category'] = match.group(4).strip()
                    if debug:
                        print(f"Found hierarchy via pattern matching: {hierarchy}")
                    break
        
        # Method 4: Look for span elements or other text containers
        if not any(hierarchy.values()[1:]):
            if debug:
                print("Trying span/div text extraction...")
            
            # Look for elements containing "Industries" and nearby text
            industries_elements = soup.find_all(text=re.compile(r'Industries', re.IGNORECASE))
            for element in industries_elements:
                parent = element.parent
                if parent:
                    # Get all text from this element and siblings
                    text_content = parent.get_text(separator=' > ', strip=True)
                    if 'industries' in text_content.lower() and '>' in text_content:
                        parts = [part.strip() for part in text_content.split('>')]
                        if debug:
                            print(f"Found text with separators: {parts}")
                        
                        # Find Industries and extract what follows
                        for i, part in enumerate(parts):
                            if part.lower() == 'industries' and i < len(parts) - 1:
                                remaining = parts[i+1:]
                                if len(remaining) >= 1:
                                    hierarchy['Driving_Category'] = remaining[0]
                                if len(remaining) >= 2:
                                    hierarchy['Category'] = remaining[1]
                                if len(remaining) >= 3:
                                    hierarchy['Sector'] = remaining[2]
                                if len(remaining) >= 4:
                                    hierarchy['Sub_Category'] = remaining[3]
                                break
                        
                        if any(hierarchy.values()[1:]):
                            break
        
        if debug:
            print(f"Final extracted hierarchy: {hierarchy}")
        
    except Exception as e:
        if debug:
            print(f"Error extracting hierarchy: {str(e)}")
        else:
            print(f"Error extracting hierarchy: {str(e)}")
    
    return hierarchy

def extract_company_data(soup, industry_name, hierarchy, debug=False):
    """
    Extract company data table from the industry page
    """
    companies_data = []
    
    if debug:
        print("\nüîç DEBUG: Starting company data extraction...")
    
    try:
        # Look for the main data table
        table = soup.find('table')
        if not table:
            # Try other selectors
            table = soup.select_one('table.data-table') or soup.select_one('table.responsive-holder') or soup.select_one('.table')
        
        if not table:
            if debug:
                print("No table found, trying div-based structure...")
            # Sometimes the data might be in div structures instead of table
            # Look for repeating row patterns
            return companies_data
        
        if debug:
            print("Found table structure")
        
        # Extract headers
        headers = []
        thead = table.find('thead')
        if thead:
            header_row = thead.find('tr')
            if header_row:
                for th in header_row.find_all(['th', 'td']):
                    header_text = th.get_text(strip=True)
                    if header_text:
                        headers.append(header_text)
        
        if not headers:
            # Try to find headers in first row of tbody
            tbody = table.find('tbody')
            if tbody:
                first_row = tbody.find('tr')
                if first_row:
                    for td in first_row.find_all(['th', 'td']):
                        header_text = td.get_text(strip=True)
                        if header_text and not header_text.replace('.', '').replace(',', '').isdigit():
                            headers.append(header_text)
        
        if debug:
            print(f"Found headers: {headers}")
        
        if not headers:
            if debug:
                print("No headers found")
            return companies_data
        
        # Extract data rows
        tbody = table.find('tbody') or table
        rows = tbody.find_all('tr')
        
        for i, row in enumerate(rows):
            cells = row.find_all(['td', 'th'])
            if len(cells) == 0:
                continue
                
            # Skip header rows
            cell_texts = [cell.get_text(strip=True) for cell in cells]
            if not cell_texts or all(text in headers for text in cell_texts if text):
                continue
            
            # Create row data
            row_data = {
                'Industry_Name': industry_name,
                'Industry': hierarchy['Industry'],
                'Driving_Category': hierarchy['Driving_Category'],
                'Category': hierarchy['Category'],
                'Sector': hierarchy['Sector'],
                'Sub_Category': hierarchy['Sub_Category']
            }
            
            # Map cell data to headers
            for j, cell in enumerate(cells):
                if j < len(headers):
                    header = headers[j]
                    cell_text = cell.get_text(strip=True)
                    
                    # Clean up common header names
                    clean_header = header.replace('.', '').replace('Rs.', 'Rs').replace('Rs.Cr.', 'Rs_Cr').replace('%', 'Percent').replace(' ', '_')
                    row_data[clean_header] = cell_text
            
            if debug and i == 0:
                print(f"Sample row data: {row_data}")
            
            companies_data.append(row_data)
        
        if debug:
            print(f"Extracted {len(companies_data)} companies")
            
    except Exception as e:
        if debug:
            print(f"Error extracting company data: {str(e)}")
        else:
            print(f"Error extracting company data: {str(e)}")
    
    return companies_data

def test_single_industry_with_companies(base_url, headers):
    """
    Test extraction on a single industry including company data
    """
    market_url = "https://www.screener.in/market/"
    
    try:
        print("üîç Fetching the market page to find the first industry...")
        response = requests.get(market_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find the first valid industry link
        industry_links = soup.find_all('a', href=re.compile(r'/market/'))
        
        first_industry = None
        for link in industry_links:
            href = link.get('href')
            industry_name = link.get_text(strip=True)
            
            # Skip sorting/header links
            if '?' in href:
                continue
                
            # Skip short names or invalid links
            if not industry_name or len(industry_name) < 2:
                continue
                
            # Skip just "market" link
            if industry_name.lower() == 'market':
                continue
                
            path_segments = href.strip('/').split('/')
            if len(path_segments) >= 2:
                first_industry = {
                    'name': industry_name,
                    'url': urljoin(base_url, href),
                    'href': href
                }
                break
        
        if not first_industry:
            print("‚ùå Could not find any valid industry link to test")
            return None
        
        print(f"\nüéØ Testing with first industry: {first_industry['name']}")
        print(f"   URL: {first_industry['url']}")
        
        # Fetch the industry page
        print("   Fetching industry page...")
        industry_response = requests.get(first_industry['url'], headers=headers, timeout=30)
        industry_response.raise_for_status()
        
        industry_soup = BeautifulSoup(industry_response.content, 'html.parser')
        
        # Extract hierarchy with debug mode
        hierarchy = extract_breadcrumb_hierarchy(industry_soup, debug=True)
        
        # Extract company data with debug mode
        companies_data = extract_company_data(industry_soup, first_industry['name'], hierarchy, debug=True)
        
        return {
            'industry': first_industry,
            'hierarchy': hierarchy,
            'companies_data': companies_data
        }
        
    except Exception as e:
        print(f"‚ùå Error testing single industry: {str(e)}")
        return None

def scrape_all_industries_and_companies(excel_filename):
    """
    Scrape all industries and their company data after validation
    """
    base_url = "https://www.screener.in"
    market_url = "https://www.screener.in/market/"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        print("\n" + "="*70)
        print("üöÄ STARTING FULL INDUSTRY & COMPANY DATA SCRAPING")
        print("="*70)
        
        # Send GET request to the market page
        print("Fetching industry list from screener.in/market/...")
        response = requests.get(market_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parse the HTML content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find all industry links
        all_company_data = []
        industry_links = soup.find_all('a', href=re.compile(r'/market/'))
        
        print(f"Found {len(industry_links)} potential industry links...")
        
        # Filter out the column header links and get only industry links
        seen_industries = set()
        processed_count = 0
        total_companies = 0
        
        for link in industry_links:
            href = link.get('href')
            industry_name = link.get_text(strip=True)
            
            # Skip if it's a sorting/header link
            if '?' in href:
                continue
                
            # Skip if it's not an industry link
            path_segments = href.strip('/').split('/')
            if len(path_segments) < 2:
                continue
                
            # Skip empty or very short names
            if not industry_name or len(industry_name) < 2:
                continue
                
            # Skip if it's just "market" link
            if industry_name.lower() == 'market':
                continue
                
            # Create full URL
            full_url = urljoin(base_url, href)
            
            # Avoid duplicates
            if industry_name not in seen_industries:
                seen_industries.add(industry_name)
                processed_count += 1
                
                # Visit the industry page to extract hierarchy and companies
                try:
                    print(f"\n[{processed_count}] Processing: {industry_name}")
                    print(f"    URL: {full_url}")
                    
                    industry_response = requests.get(full_url, headers=headers, timeout=30)
                    industry_response.raise_for_status()
                    industry_soup = BeautifulSoup(industry_response.content, 'html.parser')
                    
                    # Extract hierarchy information
                    hierarchy = extract_breadcrumb_hierarchy(industry_soup, debug=False)
                    hierarchy_display = " > ".join([h for h in [hierarchy['Driving_Category'], hierarchy['Category'], hierarchy['Sector'], hierarchy['Sub_Category']] if h])
                    print(f"    üìÇ Hierarchy: {hierarchy_display}")
                    
                    # Extract company data
                    companies_data = extract_company_data(industry_soup, industry_name, hierarchy, debug=False)
                    
                    if companies_data:
                        all_company_data.extend(companies_data)
                        total_companies += len(companies_data)
                        print(f"    ‚úÖ Found {len(companies_data)} companies (Total so far: {total_companies})")
                    else:
                        print(f"    ‚ö†Ô∏è No companies found")
                    
                    # Be nice to the server
                    time.sleep(2)
                    
                except Exception as e:
                    print(f"    ‚ùå Error processing {industry_name}: {str(e)}")
                    continue
        
        if not all_company_data:
            print("‚ùå No company data was extracted!")
            return None
        
        print(f"\nüéâ DATA EXTRACTION COMPLETED!")
        print(f"   üìä Total industries processed: {processed_count}")
        print(f"   üè¢ Total companies extracted: {len(all_company_data)}")
        
        # Create DataFrame
        df = pd.DataFrame(all_company_data)
        
        # Add a global serial number
        df['Global_S_No'] = range(1, len(df) + 1)
        
        # Reorder columns to put identifiers first
        identifier_columns = ['Global_S_No', 'Industry_Name', 'Industry', 'Driving_Category', 'Category', 'Sector', 'Sub_Category']
        other_columns = [col for col in df.columns if col not in identifier_columns]
        final_columns = identifier_columns + other_columns
        
        # Only keep columns that exist in the dataframe
        final_columns = [col for col in final_columns if col in df.columns]
        df = df[final_columns]
        
        # Save to Excel file with multiple sheets
        print(f"\nüíæ Saving data to: {excel_filename}")
        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
            # Main sheet with all data
            df.to_excel(writer, sheet_name='All_Companies_Data', index=False)
            
            # Summary sheet by industry
            summary_df = df.groupby(['Industry_Name', 'Driving_Category', 'Category', 'Sector', 'Sub_Category']).size().reset_index(name='Company_Count')
            summary_df['Summary_S_No'] = range(1, len(summary_df) + 1)
            summary_df = summary_df[['Summary_S_No'] + [col for col in summary_df.columns if col != 'Summary_S_No']]
            summary_df.to_excel(writer, sheet_name='Industry_Summary', index=False)
            
            # Format the main sheet
            workbook = writer.book
            
            # Format All_Companies_Data sheet
            if 'All_Companies_Data' in writer.sheets:
                worksheet = writer.sheets['All_Companies_Data']
                
                # Adjust column widths for main sheet
                column_widths = {
                    'A': 12,  # Global_S_No
                    'B': 35,  # Industry_Name
                    'C': 12,  # Industry
                    'D': 25,  # Driving_Category
                    'E': 35,  # Category
                    'F': 20,  # Sector
                    'G': 20,  # Sub_Category
                }
                
                # Set widths for first 7 columns, then auto-width for others
                for col_letter, width in column_widths.items():
                    worksheet.column_dimensions[col_letter].width = width
                
                # Auto-width for company data columns
                from openpyxl.utils import get_column_letter
                for i in range(8, min(len(df.columns) + 1, 20)):  # Limit to prevent too wide columns
                    col_letter = get_column_letter(i)
                    worksheet.column_dimensions[col_letter].width = 15
                
                # Add formatting
                from openpyxl.styles import Font, PatternFill
                
                header_font = Font(bold=True, color='FFFFFF')
                header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
                
                for cell in worksheet[1]:
                    cell.font = header_font
                    cell.fill = header_fill
            
            # Format Industry_Summary sheet
            if 'Industry_Summary' in writer.sheets:
                summary_worksheet = writer.sheets['Industry_Summary']
                
                summary_widths = {
                    'A': 12,  # Summary_S_No
                    'B': 35,  # Industry_Name  
                    'C': 25,  # Driving_Category
                    'D': 35,  # Category
                    'E': 20,  # Sector
                    'F': 20,  # Sub_Category
                    'G': 15,  # Company_Count
                }
                
                for col_letter, width in summary_widths.items():
                    summary_worksheet.column_dimensions[col_letter].width = width
                
                # Header formatting for summary
                for cell in summary_worksheet[1]:
                    cell.font = header_font
                    cell.fill = header_fill
        
        print(f"\n‚úÖ SUCCESS! Complete dataset saved!")
        print(f"üìä Excel file: {excel_filename}")
        print(f"üìã File contains:")
        print(f"   ‚Ä¢ Sheet 1: 'All_Companies_Data' - {len(df)} companies with full details")
        print(f"   ‚Ä¢ Sheet 2: 'Industry_Summary' - {len(summary_df)} industry categories")
        print(f"\nüîç Sample of final dataset:")
        
        # Show sample data
        sample_df = df.head(3)
        for idx, row in sample_df.iterrows():
            print(f"\n{int(row['Global_S_No'])}. Company: {row.get('Name', 'N/A')}")
            print(f"   Industry: {row['Industry_Name']}")
            print(f"   Hierarchy: {row['Industry']} > {row['Driving_Category']} > {row['Category']} > {row['Sector']} > {row['Sub_Category']}")
            # Show a few financial metrics if available
            financial_cols = [col for col in row.index if any(term in col.lower() for term in ['cmp', 'price', 'market', 'cap'])][:3]
            for col in financial_cols:
                if pd.notna(row[col]) and row[col] != '':
                    print(f"   {col}: {row[col]}")
        
        print(f"\nüéä SCRAPING COMPLETED SUCCESSFULLY!")
        return df
        
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error fetching data: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Error processing data: {e}")
        return None

def validate_extraction_with_companies():
    """
    Test the extraction method including company data and get user validation
    """
    base_url = "https://www.screener.in"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    while True:
        print("\n" + "="*70)
        print("üß™ TESTING BREADCRUMB & COMPANY DATA EXTRACTION")
        print("="*70)
        
        test_result = test_single_industry_with_companies(base_url, headers)
        
        if not test_result:
            return False
        
        industry = test_result['industry']
        hierarchy = test_result['hierarchy']
        companies_data = test_result['companies_data']
        
        # Display hierarchy results
        print(f"\nüìã HIERARCHY EXTRACTION RESULTS for '{industry['name']}':")
        print("-" * 50)
        print(f"Industry Name: {industry['name']}")
        print(f"Industry URL:  {industry['url']}")
        print("\nüèóÔ∏è EXTRACTED HIERARCHY:")
        print(f"Industry:         {hierarchy['Industry']}")
        print(f"Driving_Category: {hierarchy['Driving_Category'] if hierarchy['Driving_Category'] else '(Not found)'}")
        print(f"Category:         {hierarchy['Category'] if hierarchy['Category'] else '(Not found)'}")
        print(f"Sector:           {hierarchy['Sector'] if hierarchy['Sector'] else '(Not found)'}")
        print(f"Sub_Category:     {hierarchy['Sub_Category'] if hierarchy['Sub_Category'] else '(Not found)'}")
        
        # Display company data results
        print(f"\nüìä COMPANY DATA EXTRACTION RESULTS:")
        print("-" * 50)
        print(f"Total companies found: {len(companies_data)}")
        
        if companies_data:
            # Create DataFrame to show sample
            df_sample = pd.DataFrame(companies_data)
            print(f"\nüîç SAMPLE DATA (showing first 3 companies):")
            print("\nColumns found:", list(df_sample.columns))
            
            # Show first few rows in a readable format
            for i, row in df_sample.head(3).iterrows():
                print(f"\nCompany {i+1}:")
                for col, val in row.items():
                    if col not in ['Industry_Name', 'Industry', 'Driving_Category', 'Category', 'Sector', 'Sub_Category']:
                        print(f"  {col}: {val}")
        else:
            print("‚ö†Ô∏è No company data found!")
        
        # Ask for validation
        print("\n" + "="*70)
        while True:
            user_input = input("‚úÖ Is this extraction (hierarchy + company data) CORRECT? (yes/no/quit): ").strip().lower()
            
            if user_input in ['yes', 'y']:
                print("‚úÖ Great! The extraction method is validated.")
                return True, companies_data[0] if companies_data else None  # Return sample for column structure
            elif user_input in ['no', 'n']:
                print("üîÑ Let me try again with improved extraction...")
                break  # Go back to the outer loop to try again
            elif user_input in ['quit', 'q', 'exit']:
                print("üëã Exiting validation process.")
                return False, None
            else:
                print("Please enter 'yes', 'no', or 'quit'")

def main():
    """
    Main function to run the scraper with validation including company data
    """
    print("üöÄ Screener.in Industry & Company Data Scraper")
    print("="*70)
    print("This scraper will:")
    print("1. Test breadcrumb + company data extraction on the first industry")
    print("2. Show you the complete data structure")
    print("3. Ask you to validate the extraction")
    print("4. If correct, provide option to scrape all industries")
    print("="*70)
    
    # Step 1: Validate extraction method
    is_valid, sample_data = validate_extraction_with_companies()
    
    if not is_valid:
        print("‚ùå Validation failed or cancelled. Exiting.")
        return
    
    # Step 2: Show what the final data structure will look like
    if sample_data:
        print("\nüìä FINAL DATA STRUCTURE PREVIEW:")
        print("="*50)
        df_preview = pd.DataFrame([sample_data])
        print("Columns that will be in the final Excel:")
        for i, col in enumerate(df_preview.columns, 1):
            print(f"{i:2}. {col}")
        
        print(f"\nSample row preview:")
        for col, val in sample_data.items():
            print(f"  {col}: {val}")
    
    print(f"\n‚úÖ Validation completed successfully!")
    print(f"üí° You can now modify the code to scrape all industries,")
    print(f"   or use this structure to build your complete dataset.")
    
    # Ask if they want to proceed with full scraping
    while True:
        proceed = input("\nüöÄ Do you want me to add the full scraping functionality? (yes/no): ").strip().lower()
        if proceed in ['yes', 'y']:
            print("Ready to implement full scraping functionality!")
            break
        elif proceed in ['no', 'n']:
            print("üëç No problem! You can use this validation to understand the data structure.")
            break
        else:
            print("Please enter 'yes' or 'no'")

if __name__ == "__main__":
    main()
