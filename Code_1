import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from urllib.parse import urljoin, urlparse
import time
import json

class ScreenerScraper:
    def __init__(self):
        self.base_url = "https://www.screener.in"
        self.session = requests.Session()
        self.logged_in = False
        
        # Enhanced headers to mimic a real browser
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        
        self.session.headers.update(self.headers)
    
    def login(self, email="Dhruvpuri1346@gmail.com", password="Naveen41118"):
        """
        Login to Screener.in with the provided credentials
        """
        print("\nüîê Starting login process...")
        
        try:
            # First, get the login page to extract any required tokens/forms
            login_url = f"{self.base_url}/login/"
            print(f"   Fetching login page: {login_url}")
            
            response = self.session.get(login_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for login form
            login_form = soup.find('form')
            if not login_form:
                print("   ‚ùå Could not find login form")
                return False
            
            # Extract CSRF token if present
            csrf_token = None
            csrf_input = soup.find('input', {'name': 'csrfmiddlewaretoken'})
            if csrf_input:
                csrf_token = csrf_input.get('value')
                print(f"   Found CSRF token: {csrf_token[:20]}...")
            
            # Prepare login data
            login_data = {
                'username': email,
                'password': password,
            }
            
            if csrf_token:
                login_data['csrfmiddlewaretoken'] = csrf_token
            
            # Extract form action URL
            form_action = login_form.get('action', '/login/')
            if not form_action.startswith('http'):
                form_action = urljoin(self.base_url, form_action)
            
            print(f"   Submitting login to: {form_action}")
            
            # Add referer header for login
            login_headers = self.headers.copy()
            login_headers['Referer'] = login_url
            login_headers['Origin'] = self.base_url
            
            # Submit login
            login_response = self.session.post(
                form_action,
                data=login_data,
                headers=login_headers,
                timeout=30,
                allow_redirects=True
            )
            
            print(f"   Login response status: {login_response.status_code}")
            print(f"   Final URL after login: {login_response.url}")
            
            # Check if login was successful
            # Method 1: Check if we're redirected to dashboard or market page
            if 'login' not in login_response.url.lower():
                print("   ‚úÖ Login successful (redirected away from login page)")
                self.logged_in = True
                return True
            
            # Method 2: Check the response content for success indicators
            login_soup = BeautifulSoup(login_response.content, 'html.parser')
            
            # Look for error messages
            error_messages = login_soup.find_all(['div', 'span', 'p'], class_=re.compile(r'error|alert|warning', re.I))
            if error_messages:
                error_text = ' '.join([msg.get_text(strip=True) for msg in error_messages])
                print(f"   ‚ö†Ô∏è Possible error messages: {error_text}")
            
            # Look for success indicators (user menu, logout link, etc.)
            success_indicators = [
                login_soup.find('a', href='/logout/'),
                login_soup.find('a', text=re.compile(r'logout|sign out', re.I)),
                login_soup.find_all('a', href=re.compile(r'/screens/|/market/')),
            ]
            
            if any(success_indicators):
                print("   ‚úÖ Login successful (found user session indicators)")
                self.logged_in = True
                return True
            
            # Method 3: Try to access a protected page
            test_url = f"{self.base_url}/market/"
            test_response = self.session.get(test_url, timeout=30)
            
            if test_response.status_code == 200:
                test_soup = BeautifulSoup(test_response.content, 'html.parser')
                
                # Look for "Edit Columns" button or similar authenticated features
                edit_columns = test_soup.find('button', text=re.compile(r'edit.*column', re.I))
                if edit_columns:
                    print("   ‚úÖ Login successful (can access authenticated features)")
                    self.logged_in = True
                    return True
            
            print("   ‚ùå Login may have failed - proceeding anyway")
            return False
            
        except Exception as e:
            print(f"   ‚ùå Login error: {str(e)}")
            return False
    
    def extract_breadcrumb_hierarchy(self, soup, debug=False):
        """
        Enhanced breadcrumb extraction with login-aware features
        """
        hierarchy = {
            'Industry': 'Industries',
            'Driving_Category': '',
            'Category': '',
            'Sector': '',
            'Sub_Category': ''
        }
        
        if debug:
            print("\nüîç DEBUG: Starting breadcrumb extraction...")
        
        try:
            # Method 1: Look for breadcrumb navigation containers with various selectors
            breadcrumb_selectors = [
                'nav[aria-label*="breadcrumb"]',
                '.breadcrumb',
                'ol.breadcrumb',
                'ul.breadcrumb',
                'div.breadcrumb',
                '.nav-breadcrumb',
                '.breadcrumbs',
                'nav',
                '.navigation-path',
                '.path-navigation'
            ]
            
            breadcrumb_container = None
            
            for selector in breadcrumb_selectors:
                containers = soup.select(selector)
                for container in containers:
                    container_text = container.get_text().lower()
                    if 'industries' in container_text:
                        breadcrumb_container = container
                        if debug:
                            print(f"Found breadcrumb container using selector: {selector}")
                        break
                if breadcrumb_container:
                    break
            
            # Method 2: Look for page title or heading that might contain hierarchy
            if not breadcrumb_container:
                title_elements = soup.find_all(['h1', 'h2', 'h3', '.page-title', '.section-title'])
                for title in title_elements:
                    title_text = title.get_text(strip=True)
                    if len(title_text) > 5 and any(word in title_text.lower() for word in ['companies', 'industry', 'sector']):
                        # This might be our industry name
                        hierarchy['Sub_Category'] = title_text
                        if debug:
                            print(f"Found potential sub-category from title: {title_text}")
            
            # Method 3: Enhanced link-based extraction
            if breadcrumb_container:
                links = breadcrumb_container.find_all('a')
                text_items = []
                
                for link in links:
                    text = link.get_text(strip=True)
                    if text and text.lower() not in ['home', 'login', 'screens', 'dashboard', '']:
                        text_items.append(text)
                
                # Also look for non-link text in breadcrumbs
                all_text = breadcrumb_container.get_text(separator=' > ', strip=True)
                if '>' in all_text:
                    text_parts = [part.strip() for part in all_text.split('>')]
                    text_parts = [part for part in text_parts if part and part.lower() not in ['home', 'login', 'screens', 'dashboard']]
                    if len(text_parts) > len(text_items):
                        text_items = text_parts
                
                if debug:
                    print(f"Extracted breadcrumb items: {text_items}")
                
                # Find Industries and map hierarchy
                industries_index = -1
                for i, item in enumerate(text_items):
                    if item.lower() == 'industries':
                        industries_index = i
                        break
                
                if industries_index >= 0:
                    remaining = text_items[industries_index + 1:]
                    if len(remaining) >= 1:
                        hierarchy['Driving_Category'] = remaining[0]
                    if len(remaining) >= 2:
                        hierarchy['Category'] = remaining[1]
                    if len(remaining) >= 3:
                        hierarchy['Sector'] = remaining[2]
                    if len(remaining) >= 4:
                        hierarchy['Sub_Category'] = remaining[3]
            
            # Method 4: URL-based extraction as fallback
            if not any(hierarchy.values()[1:]):
                current_url = soup.find('meta', property='og:url')
                if current_url:
                    url = current_url.get('content', '')
                elif hasattr(soup, 'url'):
                    url = soup.url
                else:
                    url = ''
                
                if '/market/' in url:
                    # Extract from URL path
                    path_parts = url.split('/market/')[-1].split('/')
                    path_parts = [part for part in path_parts if part and part != '']
                    
                    # Map URL segments to hierarchy (this is speculative)
                    if len(path_parts) >= 1:
                        hierarchy['Sub_Category'] = path_parts[-1].replace('-', ' ').title()
            
            if debug:
                print(f"Final extracted hierarchy: {hierarchy}")
            
        except Exception as e:
            if debug:
                print(f"Error extracting hierarchy: {str(e)}")
        
        return hierarchy
    
    def extract_company_data(self, soup, industry_name, hierarchy, debug=False):
        """
        Enhanced company data extraction with login-aware features
        """
        companies_data = []
        
        if debug:
            print("\nüîç DEBUG: Starting company data extraction...")
        
        try:
            # Look for data tables with various selectors
            table_selectors = [
                'table',
                '.data-table',
                '.table',
                '.companies-table',
                '.responsive-holder table',
                '[role="table"]'
            ]
            
            table = None
            for selector in table_selectors:
                tables = soup.select(selector)
                for t in tables:
                    # Check if this table contains company data
                    table_text = t.get_text().lower()
                    if any(indicator in table_text for indicator in ['company', 'cmp', 'market cap', 'sales', 'profit']):
                        table = t
                        if debug:
                            print(f"Found data table using selector: {selector}")
                        break
                if table:
                    break
            
            if not table:
                if debug:
                    print("No suitable data table found")
                return companies_data
            
            # Extract headers
            headers = []
            
            # Try multiple methods to find headers
            header_sources = [
                table.select('thead tr th'),
                table.select('thead tr td'),
                table.select('tr:first-child th'),
                table.select('tr:first-child td'),
            ]
            
            for header_source in header_sources:
                if header_source:
                    headers = [th.get_text(strip=True) for th in header_source]
                    headers = [h for h in headers if h]  # Remove empty headers
                    if headers:
                        break
            
            if debug:
                print(f"Found headers: {headers}")
            
            if not headers:
                if debug:
                    print("No headers found")
                return companies_data
            
            # Extract data rows
            tbody = table.find('tbody')
            if not tbody:
                tbody = table
            
            rows = tbody.find_all('tr')
            
            # Skip header row if it's included in tbody
            start_index = 0
            if rows and len(rows) > 0:
                first_row_cells = [cell.get_text(strip=True) for cell in rows[0].find_all(['td', 'th'])]
                # If first row matches headers exactly, skip it
                if first_row_cells == headers:
                    start_index = 1
            
            for i, row in enumerate(rows[start_index:], start=1):
                cells = row.find_all(['td', 'th'])
                if len(cells) == 0:
                    continue
                
                # Create base row data with hierarchy info
                row_data = {
                    'Industry_Name': industry_name,
                    'Industry': hierarchy['Industry'],
                    'Driving_Category': hierarchy['Driving_Category'],
                    'Category': hierarchy['Category'],
                    'Sector': hierarchy['Sector'],
                    'Sub_Category': hierarchy['Sub_Category']
                }
                
                # Extract cell data
                for j, cell in enumerate(cells):
                    if j < len(headers):
                        header = headers[j]
                        cell_text = cell.get_text(strip=True)
                        
                        # Clean header names for column naming
                        clean_header = (header.replace('.', '')
                                             .replace('Rs.', 'Rs')
                                             .replace('Rs.Cr.', 'Rs_Cr')
                                             .replace('%', 'Percent')
                                             .replace(' ', '_')
                                             .replace('/', '_')
                                             .replace('(', '')
                                             .replace(')', ''))
                        
                        row_data[clean_header] = cell_text
                
                # Only add row if it has meaningful data (not just empty or header-like)
                non_hierarchy_data = {k: v for k, v in row_data.items() 
                                     if k not in ['Industry_Name', 'Industry', 'Driving_Category', 
                                                 'Category', 'Sector', 'Sub_Category']}
                
                if any(v for v in non_hierarchy_data.values()):
                    companies_data.append(row_data)
                    
                    if debug and i <= 3:
                        print(f"Sample row {i}: {dict(list(non_hierarchy_data.items())[:3])}")
            
            if debug:
                print(f"Extracted {len(companies_data)} companies")
                
        except Exception as e:
            if debug:
                print(f"Error extracting company data: {str(e)}")
            else:
                print(f"Error extracting company data: {str(e)}")
        
        return companies_data
    
    def get_industry_links(self):
        """
        Get all industry links from the market page
        """
        market_url = f"{self.base_url}/market/"
        
        try:
            print("üîç Fetching industry list from market page...")
            response = self.session.get(market_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find all industry links
            industry_links = soup.find_all('a', href=re.compile(r'/market/'))
            
            industries = []
            seen_names = set()
            
            for link in industry_links:
                href = link.get('href')
                industry_name = link.get_text(strip=True)
                
                # Skip invalid links
                if not href or not industry_name:
                    continue
                
                # Skip sorting/query parameters
                if '?' in href:
                    continue
                
                # Skip short names or duplicates
                if len(industry_name) < 2 or industry_name in seen_names:
                    continue
                
                # Skip generic links
                if industry_name.lower() in ['market', 'industries', 'home', 'login']:
                    continue
                
                # Validate URL structure
                path_segments = href.strip('/').split('/')
                if len(path_segments) < 2:
                    continue
                
                full_url = urljoin(self.base_url, href)
                
                industries.append({
                    'name': industry_name,
                    'url': full_url,
                    'href': href
                })
                
                seen_names.add(industry_name)
            
            print(f"   Found {len(industries)} valid industry links")
            return industries
            
        except Exception as e:
            print(f"‚ùå Error fetching industry links: {str(e)}")
            return []
    
    def test_single_industry(self, debug=True):
        """
        Test extraction on a single industry
        """
        industries = self.get_industry_links()
        
        if not industries:
            print("‚ùå No industries found to test")
            return None
        
        # Use first industry for testing
        test_industry = industries[0]
        
        print(f"\nüß™ Testing with: {test_industry['name']}")
        print(f"   URL: {test_industry['url']}")
        
        try:
            response = self.session.get(test_industry['url'], timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract hierarchy
            hierarchy = self.extract_breadcrumb_hierarchy(soup, debug=debug)
            
            # Extract company data
            companies_data = self.extract_company_data(soup, test_industry['name'], hierarchy, debug=debug)
            
            return {
                'industry': test_industry,
                'hierarchy': hierarchy,
                'companies_data': companies_data
            }
            
        except Exception as e:
            print(f"‚ùå Error testing industry: {str(e)}")
            return None
    
    def scrape_all_industries(self, excel_filename):
        """
        Scrape all industries and their company data
        """
        print("\n" + "="*70)
        print("üöÄ STARTING FULL INDUSTRY & COMPANY DATA SCRAPING")
        print("="*70)
        
        # Ensure we're logged in
        if not self.logged_in:
            print("‚ö†Ô∏è Not logged in, attempting login first...")
            if not self.login():
                print("‚ùå Login failed, proceeding without authentication...")
        
        industries = self.get_industry_links()
        
        if not industries:
            print("‚ùå No industries found!")
            return None
        
        all_company_data = []
        processed_count = 0
        total_companies = 0
        failed_industries = []
        
        print(f"üìä Processing {len(industries)} industries...")
        
        for industry in industries:
            processed_count += 1
            
            try:
                print(f"\n[{processed_count}/{len(industries)}] Processing: {industry['name']}")
                print(f"    URL: {industry['url']}")
                
                response = self.session.get(industry['url'], timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extract hierarchy
                hierarchy = self.extract_breadcrumb_hierarchy(soup, debug=False)
                hierarchy_display = " > ".join([h for h in [
                    hierarchy['Driving_Category'], 
                    hierarchy['Category'], 
                    hierarchy['Sector'], 
                    hierarchy['Sub_Category']
                ] if h])
                
                print(f"    üìÇ Hierarchy: {hierarchy_display}")
                
                # Extract company data
                companies_data = self.extract_company_data(soup, industry['name'], hierarchy, debug=False)
                
                if companies_data:
                    all_company_data.extend(companies_data)
                    total_companies += len(companies_data)
                    print(f"    ‚úÖ Found {len(companies_data)} companies (Total: {total_companies})")
                else:
                    print(f"    ‚ö†Ô∏è No companies found")
                    failed_industries.append(industry['name'])
                
                # Be respectful to the server
                time.sleep(2)
                
            except Exception as e:
                print(f"    ‚ùå Error processing {industry['name']}: {str(e)}")
                failed_industries.append(industry['name'])
                continue
        
        # Create and save DataFrame
        if all_company_data:
            print(f"\nüéâ DATA EXTRACTION COMPLETED!")
            print(f"   üìä Industries processed: {processed_count}")
            print(f"   üè¢ Total companies: {len(all_company_data)}")
            print(f"   ‚ùå Failed industries: {len(failed_industries)}")
            
            if failed_industries:
                print(f"   Failed: {', '.join(failed_industries[:5])}" + 
                      (f" and {len(failed_industries)-5} more..." if len(failed_industries) > 5 else ""))
            
            return self.save_to_excel(all_company_data, excel_filename)
        else:
            print("‚ùå No data extracted!")
            return None
    
    def save_to_excel(self, companies_data, filename):
        """
        Save data to Excel with proper formatting
        """
        try:
            df = pd.DataFrame(companies_data)
            
            # Add global serial number
            df['Global_S_No'] = range(1, len(df) + 1)
            
            # Reorder columns
            identifier_columns = ['Global_S_No', 'Industry_Name', 'Industry', 
                                'Driving_Category', 'Category', 'Sector', 'Sub_Category']
            other_columns = [col for col in df.columns if col not in identifier_columns]
            final_columns = identifier_columns + other_columns
            final_columns = [col for col in final_columns if col in df.columns]
            df = df[final_columns]
            
            print(f"\nüíæ Saving data to: {filename}")
            
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # Main data sheet
                df.to_excel(writer, sheet_name='All_Companies_Data', index=False)
                
                # Summary sheet
                summary_df = df.groupby(['Industry_Name', 'Driving_Category', 'Category', 
                                       'Sector', 'Sub_Category']).size().reset_index(name='Company_Count')
                summary_df['Summary_S_No'] = range(1, len(summary_df) + 1)
                summary_df = summary_df[['Summary_S_No'] + [col for col in summary_df.columns if col != 'Summary_S_No']]
                summary_df.to_excel(writer, sheet_name='Industry_Summary', index=False)
                
                # Format the sheets
                self._format_excel_sheets(writer, df, summary_df)
            
            print(f"‚úÖ SUCCESS! Data saved to {filename}")
            print(f"üìä Total companies: {len(df)}")
            print(f"üìÇ Industries: {len(summary_df)}")
            
            return df
            
        except Exception as e:
            print(f"‚ùå Error saving Excel file: {str(e)}")
            return None
    
    def _format_excel_sheets(self, writer, df, summary_df):
        """
        Apply formatting to Excel sheets
        """
        try:
            from openpyxl.styles import Font, PatternFill
            from openpyxl.utils import get_column_letter
            
            # Format main sheet
            if 'All_Companies_Data' in writer.sheets:
                ws = writer.sheets['All_Companies_Data']
                
                # Header formatting
                header_font = Font(bold=True, color='FFFFFF')
                header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
                
                for cell in ws[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                
                # Column widths
                column_widths = {
                    'A': 12, 'B': 35, 'C': 12, 'D': 25, 'E': 35, 'F': 20, 'G': 20
                }
                
                for col_letter, width in column_widths.items():
                    ws.column_dimensions[col_letter].width = width
                
                # Auto-width for other columns
                for i in range(8, min(len(df.columns) + 1, 20)):
                    ws.column_dimensions[get_column_letter(i)].width = 15
            
            # Format summary sheet
            if 'Industry_Summary' in writer.sheets:
                ws = writer.sheets['Industry_Summary']
                
                for cell in ws[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                
                summary_widths = {'A': 12, 'B': 35, 'C': 25, 'D': 35, 'E': 20, 'F': 20, 'G': 15}
                for col_letter, width in summary_widths.items():
                    ws.column_dimensions[col_letter].width = width
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not apply Excel formatting: {str(e)}")

def validate_extraction():
    """
    Test and validate the extraction process
    """
    scraper = ScreenerScraper()
    
    while True:
        print("\n" + "="*70)
        print("üß™ TESTING ENHANCED SCRAPER WITH LOGIN")
        print("="*70)
        
        # Login first
        if not scraper.login():
            print("‚ùå Login failed. Continue anyway? (y/n): ", end="")
            if input().strip().lower() != 'y':
                return False
        
        # Test extraction
        test_result = scraper.test_single_industry(debug=True)
        
        if not test_result:
            print("‚ùå Test failed!")
            return False
        
        industry = test_result['industry']
        hierarchy = test_result['hierarchy']
        companies_data = test_result['companies_data']
        
        # Display results
        print(f"\nüìã RESULTS for '{industry['name']}':")
        print("-" * 50)
        print(f"üèóÔ∏è HIERARCHY:")
        for key, value in hierarchy.items():
            print(f"   {key}: {value if value else '(Not found)'}")
        
        print(f"\nüìä COMPANY DATA:")
        print(f"   Total companies: {len(companies_data)}")
        
        if companies_data:
            df_sample = pd.DataFrame(companies_data)
            print(f"   Columns found: {list(df_sample.columns)}")
            
            print(f"\nüîç SAMPLE COMPANIES (first 3):")
            for i, row in df_sample.head(3).iterrows():
                print(f"\n   Company {i+1}:")
                for col, val in row.items():
                    if col not in ['Industry_Name', 'Industry', 'Driving_Category', 
                                 'Category', 'Sector', 'Sub_Category']:
                        print(f"     {col}: {val}")
        
        # Validation
        print("\n" + "="*70)
        while True:
            response = input("‚úÖ Is this extraction correct? (yes/no/quit): ").strip().lower()
            
            if response in ['yes', 'y']:
                return True
            elif response in ['no', 'n']:
                print("üîÑ Trying again...")
                break
            elif response in ['quit', 'q', 'exit']:
                return False
            else:
                print("Please enter 'yes', 'no', or 'quit'")

def main():
    """
    Main function to run the enhanced scraper
    """
    print("üöÄ Enhanced Screener.in Scraper with Login Authentication")
    print("="*70)
    
    # Validate extraction
    if not validate_extraction():
        print("‚ùå Validation failed. Exiting.")
        return
    
    # Get filename
    while True:
        filename = input("\nüìÅ Enter Excel filename (e.g., 'screener_data.xlsx'): ").strip()
        
        if not filename:
            print("Please enter a valid filename.")
            continue
        
        if not filename.lower().endswith('.xlsx'):
            filename += '.xlsx'
        
        print(f"‚úÖ Will save to: {filename}")
        break
    
    # Start full scraping
    scraper = ScreenerScraper()
    
    print(f"\nüöÄ Starting full extraction...")
    df = scraper.scrape_all_industries(filename)
    
    if df is not None:
        print(f"\nüéâ SCRAPING COMPLETED SUCCESSFULLY!")
        print(f"üìä Final dataset contains {len(df)} companies")
        
        # Show sample data
        print(f"\nüîç Sample of extracted data:")
        for idx, row in df.head(3).iterrows():
            company_name = next((row[col] for col in row.index 
                               if 'name' in col.lower() and row[col]), 'Unknown')
            print(f"\n{int(row['Global_S_No'])}. {company_name}")
            print(f"   Industry: {row['Industry_Name']}")
            print(f"   Hierarchy: {row['Driving_Category']} > {row['Category']} > {row['Sector']} > {row['Sub_Category']}")
    else:
        print(f"\n‚ùå Scraping failed.")

if __name__ == "__main__":
    main()
