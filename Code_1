import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from urllib.parse import urljoin

def scrape_screener_industries():
    """
    Scrapes all industry links from screener.in/market/ and saves to Excel
    """
    
    # Base URL
    base_url = "https://www.screener.in"
    market_url = "https://www.screener.in/market/"
    
    # Headers to mimic a real browser (Yahoo-like user agent as requested)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        # Send GET request to the market page
        print("Fetching data from screener.in/market/...")
        response = requests.get(market_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parse the HTML content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find the table containing industry data
        # The industries are in a table structure
        industry_data = []
        
        # Find all rows in the table
        # Looking for links that contain "/market/" in their href
        industry_links = soup.find_all('a', href=re.compile(r'/market/'))
        
        print(f"Found {len(industry_links)} potential industry links...")
        
        # Filter out the column header links and get only industry links
        seen_industries = set()
        
        for link in industry_links:
            href = link.get('href')
            industry_name = link.get_text(strip=True)
            
            # Skip if it's a sorting/header link (contains query parameters)
            if '?' in href:
                continue
                
            # Skip if it's not an industry link (should have multiple path segments)
            path_segments = href.strip('/').split('/')
            if len(path_segments) < 4:  # market/XX/XXXX/XXXXXXXX/XXXXXXXXXX
                continue
                
            # Skip empty or very short names
            if not industry_name or len(industry_name) < 2:
                continue
                
            # Create full URL
            full_url = urljoin(base_url, href)
            
            # Avoid duplicates
            if industry_name not in seen_industries:
                seen_industries.add(industry_name)
                industry_data.append({
                    'Industry_Name': industry_name,
                    'Industry_Link': full_url
                })
                print(f"Added: {industry_name}")
        
        # Sort by industry name for better organization
        industry_data.sort(key=lambda x: x['Industry_Name'])
        
        # Create DataFrame
        df = pd.DataFrame(industry_data)
        
        # Add some additional columns that might be useful
        df['Serial_No'] = range(1, len(df) + 1)
        df = df[['Serial_No', 'Industry_Name', 'Industry_Link']]  # Reorder columns
        
        # Save to Excel file
        excel_filename = 'screener_industry_links.xlsx'
        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Industry_Links', index=False)
            
            # Get the workbook and worksheet objects
            workbook = writer.book
            worksheet = writer.sheets['Industry_Links']
            
            # Adjust column widths for better readability
            worksheet.column_dimensions['A'].width = 12  # Serial_No
            worksheet.column_dimensions['B'].width = 50  # Industry_Name
            worksheet.column_dimensions['C'].width = 80  # Industry_Link
            
            # Add some basic formatting
            from openpyxl.styles import Font, PatternFill
            
            # Header formatting
            header_font = Font(bold=True, color='FFFFFF')
            header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
            
            for cell in worksheet[1]:
                cell.font = header_font
                cell.fill = header_fill
        
        print(f"\nâœ… Successfully scraped {len(industry_data)} industries!")
        print(f"ðŸ“Š Data saved to: {excel_filename}")
        print("\nðŸ“‹ Summary:")
        print(f"   â€¢ Total industries found: {len(industry_data)}")
        print(f"   â€¢ Excel file: {excel_filename}")
        print(f"   â€¢ Columns: Serial_No, Industry_Name, Industry_Link")
        
        # Display first few entries as preview
        print("\nðŸ” Preview of first 5 industries:")
        for i, item in enumerate(industry_data[:5]):
            print(f"{i+1:2d}. {item['Industry_Name']}")
            print(f"    Link: {item['Industry_Link']}")
            print()
        
        return df
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Error fetching data: {e}")
        return None
    except Exception as e:
        print(f"âŒ Error processing data: {e}")
        return None

# Additional function to verify links
def verify_sample_links(df, sample_size=5):
    """
    Verify a sample of scraped links to ensure they work
    """
    import random
    
    if df is None or len(df) == 0:
        print("No data to verify")
        return
    
    print(f"\nðŸ” Verifying {sample_size} random industry links...")
    
    # Select random sample
    sample_indices = random.sample(range(len(df)), min(sample_size, len(df)))
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for i, idx in enumerate(sample_indices):
        industry = df.iloc[idx]
        print(f"\n{i+1}. Testing: {industry['Industry_Name']}")
        print(f"   URL: {industry['Industry_Link']}")
        
        try:
            response = requests.get(industry['Industry_Link'], headers=headers, timeout=10)
            if response.status_code == 200:
                print("   âœ… Link works!")
            else:
                print(f"   âŒ Status code: {response.status_code}")
        except Exception as e:
            print(f"   âŒ Error: {str(e)[:50]}...")

if __name__ == "__main__":
    # Run the scraper
    print("ðŸš€ Starting Screener.in Industry Links Scraper...")
    print("=" * 60)
    
    # Scrape the data
    df = scrape_screener_industries()
    
    # Verify some links
    if df is not None:
        verify_sample_links(df, sample_size=3)
    
    print("\n" + "=" * 60)
    print("âœ¨ Scraping completed!")
